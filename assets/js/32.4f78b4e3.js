(window.webpackJsonp=window.webpackJsonp||[]).push([[32],{1114:function(s,r,a){s.exports=a.p+"assets/img/1.3.1.爬虫流程-1.05924f02.png"},1115:function(s,r,a){s.exports=a.p+"assets/img/1.3.2.爬虫流程-2.d145a538.png"},1116:function(s,r,a){s.exports=a.p+"assets/img/1.3.4.scrapy组件.84ecd23b.png"},1323:function(s,r,a){"use strict";a.r(r);var e=a(3),t=Object(e.a)({},(function(){var s=this,r=s.$createElement,e=s._self._c||r;return e("ContentSlotsDistributor",{attrs:{"slot-key":s.$parent.slotKey}},[e("p",[s._v("介绍")]),s._v(" "),e("p",[s._v("我们知道常用的流程web框架有django、flask，那么接下来，我们会来学习一个全世界范围最流行的爬虫框架scrapy")]),s._v(" "),e("p",[s._v("内容")]),s._v(" "),e("ul",[e("li",[s._v("scrapy的概念作用和工作流程")]),s._v(" "),e("li",[s._v("scrapy的入门使用")]),s._v(" "),e("li",[s._v("scrapy构造并发送请求")]),s._v(" "),e("li",[s._v("scrapy模拟登陆")]),s._v(" "),e("li",[s._v("scrapy管道的使用")]),s._v(" "),e("li",[s._v("scrapy中间件的使用")]),s._v(" "),e("li",[s._v("scrapy_redis概念作用和流程")]),s._v(" "),e("li",[s._v("scrapy_redis原理分析并实现断点续爬以及分布式爬虫")]),s._v(" "),e("li",[s._v("scrapy_splash组件的使用")]),s._v(" "),e("li",[s._v("scrapy的日志信息与配置")]),s._v(" "),e("li",[s._v("scrapyd部署scrapy项目")])]),s._v(" "),e("p",[s._v("scrapy官方文档")]),s._v(" "),e("blockquote",[e("p",[s._v("https://scrapy-chs.readthedocs.io/zh_CN/0.24/index.html")])]),s._v(" "),e("h2",{attrs:{id:"scrapy的概念和流程"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#scrapy的概念和流程"}},[s._v("#")]),s._v(" scrapy的概念和流程")]),s._v(" "),e("h5",{attrs:{id:"学习目标"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#学习目标"}},[s._v("#")]),s._v(" 学习目标：")]),s._v(" "),e("ol",[e("li",[s._v("了解 scrapy的概念")]),s._v(" "),e("li",[s._v("了解 scrapy框架的作用")]),s._v(" "),e("li",[s._v("掌握 scrapy框架的运行流程")]),s._v(" "),e("li",[s._v("掌握 scrapy中每个模块的作用")])]),s._v(" "),e("hr"),s._v(" "),e("h3",{attrs:{id:"_1-scrapy的概念"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_1-scrapy的概念"}},[s._v("#")]),s._v(" 1. scrapy的概念")]),s._v(" "),e("p",[e("strong",[s._v("Scrapy是一个Python编写的开源网络爬虫框架。它是一个被设计用于爬取网络数据、提取结构性数据的框架。")])]),s._v(" "),e("blockquote",[e("p",[s._v("Scrapy 使用了Twisted['twɪstɪd]异步网络框架，可以加快我们的下载速度。")])]),s._v(" "),e("blockquote",[e("p",[s._v("Scrapy文档地址：http://scrapy-chs.readthedocs.io/zh_CN/1.0/intro/overview.html")])]),s._v(" "),e("h3",{attrs:{id:"_2-scrapy框架的作用"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_2-scrapy框架的作用"}},[s._v("#")]),s._v(" 2. scrapy框架的作用")]),s._v(" "),e("blockquote",[e("p",[s._v("少量的代码，就能够快速的抓取")])]),s._v(" "),e("h3",{attrs:{id:"_3-scrapy的工作流程"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_3-scrapy的工作流程"}},[s._v("#")]),s._v(" 3. scrapy的工作流程")]),s._v(" "),e("h4",{attrs:{id:"_3-1-回顾之前的爬虫流程"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_3-1-回顾之前的爬虫流程"}},[s._v("#")]),s._v(" 3.1 回顾之前的爬虫流程")]),s._v(" "),e("img",{attrs:{src:a(1114),width:"80%"}}),s._v(" "),e("h4",{attrs:{id:"_3-2-上面的流程可以改写为"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_3-2-上面的流程可以改写为"}},[s._v("#")]),s._v(" 3.2 上面的流程可以改写为")]),s._v(" "),e("img",{attrs:{src:a(1115),width:"80%"}}),s._v(" "),e("h4",{attrs:{id:"_3-3-scrapy的流程"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_3-3-scrapy的流程"}},[s._v("#")]),s._v(" 3.3 scrapy的流程")]),s._v(" "),e("img",{attrs:{src:a(788),width:"140%"}}),s._v(" "),e("h5",{attrs:{id:"其流程可以描述如下"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#其流程可以描述如下"}},[s._v("#")]),s._v(" 其流程可以描述如下：")]),s._v(" "),e("ol",[e("li",[s._v("爬虫中起始的url构造成request对象--\x3e爬虫中间件--\x3e引擎--\x3e调度器")]),s._v(" "),e("li",[s._v("调度器把request--\x3e引擎--\x3e下载中间件---\x3e下载器")]),s._v(" "),e("li",[s._v("下载器发送请求，获取response响应----\x3e下载中间件----\x3e引擎---\x3e爬虫中间件---\x3e爬虫")]),s._v(" "),e("li",[s._v("爬虫提取url地址，组装成request对象----\x3e爬虫中间件---\x3e引擎---\x3e调度器，重复步骤2")]),s._v(" "),e("li",[s._v("爬虫提取数据---\x3e引擎---\x3e管道处理和保存数据")])]),s._v(" "),e("h5",{attrs:{id:"注意"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#注意"}},[s._v("#")]),s._v(" 注意：")]),s._v(" "),e("ul",[e("li",[s._v("图中中文是为了方便理解后加上去的")]),s._v(" "),e("li",[s._v("图中绿色线条的表示数据的传递")]),s._v(" "),e("li",[s._v("注意图中中间件的位置，决定了其作用")]),s._v(" "),e("li",[s._v("注意其中引擎的位置，所有的模块之前相互独立，只和引擎进行交互")])]),s._v(" "),e("h4",{attrs:{id:"_3-4-scrapy的三个内置对象"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_3-4-scrapy的三个内置对象"}},[s._v("#")]),s._v(" 3.4 scrapy的三个内置对象")]),s._v(" "),e("ul",[e("li",[s._v("request请求对象：由url method post_data headers等构成")]),s._v(" "),e("li",[s._v("response响应对象：由url body status headers等构成")]),s._v(" "),e("li",[s._v("item数据对象：本质是个字典")])]),s._v(" "),e("h4",{attrs:{id:"_3-5-scrapy中每个模块的具体作用"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_3-5-scrapy中每个模块的具体作用"}},[s._v("#")]),s._v(" 3.5 scrapy中每个模块的具体作用")]),s._v(" "),e("img",{attrs:{src:a(1116),width:"80%"}}),s._v(" "),e("h5",{attrs:{id:"注意-2"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#注意-2"}},[s._v("#")]),s._v(" 注意：")]),s._v(" "),e("ul",[e("li",[s._v("爬虫中间件和下载中间件只是运行逻辑的位置不同，作用是重复的：如替换UA等")])]),s._v(" "),e("hr"),s._v(" "),e("h2",{attrs:{id:"小结"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#小结"}},[s._v("#")]),s._v(" 小结")]),s._v(" "),e("ol",[e("li",[s._v("scrapy的概念：Scrapy是一个为了爬取网站数据，提取结构性数据而编写的应用框架")]),s._v(" "),e("li",[s._v("scrapy框架的运行流程以及数据传递过程：\n"),e("ol",[e("li",[s._v("爬虫中起始的url构造成request对象--\x3e爬虫中间件--\x3e引擎--\x3e调度器")]),s._v(" "),e("li",[s._v("调度器把request--\x3e引擎--\x3e下载中间件---\x3e下载器")]),s._v(" "),e("li",[s._v("下载器发送请求，获取response响应----\x3e下载中间件----\x3e引擎---\x3e爬虫中间件---\x3e爬虫")]),s._v(" "),e("li",[s._v("爬虫提取url地址，组装成request对象----\x3e爬虫中间件---\x3e引擎---\x3e调度器，重复步骤2")]),s._v(" "),e("li",[s._v("爬虫提取数据---\x3e引擎---\x3e管道处理和保存数据")])])]),s._v(" "),e("li",[s._v("scrapy框架的作用：通过少量代码实现快速抓取")]),s._v(" "),e("li",[s._v("掌握scrapy中每个模块的作用：\n引擎(engine)：负责数据和信号在不腰痛模块间的传递\n调度器(scheduler)：实现一个队列，存放引擎发过来的request请求对象\n下载器(downloader)：发送引擎发过来的request请求，获取响应，并将响应交给引擎\n爬虫(spider)：处理引擎发过来的response，提取数据，提取url，并交给引擎\n管道(pipeline)：处理引擎传递过来的数据，比如存储\n下载中间件(downloader middleware)：可以自定义的下载扩展，比如设置代理ip\n爬虫中间件(spider middleware)：可以自定义request请求和进行response过滤，与下载中间件作用重复")])]),s._v(" "),e("hr")])}),[],!1,null,null,null);r.default=t.exports},788:function(s,r,a){s.exports=a.p+"assets/img/1.3.3.scrapy工作流程.19eaffd9.png"}}]);